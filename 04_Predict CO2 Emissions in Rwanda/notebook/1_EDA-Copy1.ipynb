{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import pyforest\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "#test statistique : \n",
    "from scipy.stats import spearmanr, kendalltau,pearsonr, shapiro\n",
    "\n",
    "#import outils reductions dimensions\n",
    "from sklearn.decomposition import PCA #linéaire\n",
    "from sklearn.manifold import TSNE #non linéaire pour structure locale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#package modele non supervisé\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer #permet tracer courbe pour choisir bonne valeur KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc14330",
   "metadata": {},
   "source": [
    "- \n",
    "#### L'objectif de ce défi est de créer des modèles d'apprentissage automatique qui utilisent des données d'émissions de source ouverte (provenant des observations du satellite Sentinel-5P) pour prédire les émissions de carbone.\n",
    "\n",
    "* **7 features principales** ont été extraites de janvier 2019 à novembre 2022. \n",
    "* Chaque caractéristique (dioxyde de soufre, monoxyde de carbone, etc.) **contient des sous caractéristiques** telles que\n",
    "    - column_number_density qui est la densité verticale de la colonne au niveau du sol\n",
    "\n",
    "- **SulphurDioxide** : pénètre dans l'atmosphère terrestre par des processus naturels et anthropiques.\n",
    "- **CarbonMonoxide** : Dans certaines zones urbaines, il constitue un polluant atmosphérique majeur\n",
    "- **NitrogenDioxide** : NO2 est utilisé pour représenter les concentrations d'oxydes d'azote collectifs car pendant la journée, c'est-à-dire en présence de lumière solaire, un cycle photochimique impliquant l'ozone (O3) convertit NO en NO2 et vice-versa sur une échelle de temps de quelques minutes.\n",
    "- **Formaldehyde** : Le formaldéhyde est un gaz intermédiaire dans presque toutes les chaînes d'oxydation des composés organiques volatils non méthaniques (COVNM), qui aboutissent finalement au CO2.\n",
    "- **UvAerosolIndex** :l'indice d'aérosol UV (UVAI), également appelé indice d'aérosol absorbant (AAI).Lorsque l'IAA est positif, il indique la présence d'aérosols absorbant les UV, tels que la poussière et la fumée.\n",
    "- **Ozone** : Dans la stratosphère, la couche d'ozone protège la biosphère des dangereux rayonnements solaires ultraviolets. Dans la troposphère, elle agit comme un agent nettoyant efficace, mais à forte concentration, elle devient également nocive pour la santé de l'homme, des animaux et de la végétation. C'est un important gaz à effet de serre qui contribue au changement climatique\n",
    "- **Cloud** : Ce jeu de données récupères les propriétés des nuages et fournit une imagerie hors ligne à haute résolution de ses paramètres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f2cd9",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "#### 1. Connaissance du jeu de données \n",
    "- Présence d'une target ?\n",
    "- Features signification\n",
    "- Dimensions des données\n",
    "- Type de données et cohérence (variables discrètes : int, continues : float, qualitative : str)\n",
    "\n",
    "#### 2. imputation des données :\n",
    "- NaN présent : vrai NaN ou signifie une absence d'élément ?\n",
    "- Visualisation des NaN\n",
    "- Variables numériques :\n",
    "    - discrètes : remplacement par le mode \n",
    "    - continues : moyenne ou médiane en fonction des outliers\n",
    "    \n",
    "### 3. Analyse univariée\n",
    "\n",
    "### 4. Analyse bivariée & test statistiques\n",
    "- Target(continue)vs features continues : \n",
    "    - heatmap\n",
    "    - Test shapiro wilk\n",
    "    - test de corrélation : Spearman/Kendall\n",
    "\n",
    "\n",
    "### 5. Analyse multivariée\n",
    "- Analyse des features continues via heatmap\n",
    "- Analyse des variables corrélées et anticorrélées avec scatterplot\n",
    "\n",
    "### 6. PCA & tSNE : sur l'ensemble du jeu de données (cad toutes les features, mais tester sur le train set uniquement)\n",
    "#### PCA\n",
    "- Standardisation des données\n",
    "- Visualisation des données numériques\n",
    "#### tSNE\n",
    "- Visualisation des données numériques\n",
    "\n",
    "### 7. PCA & tSNE : sur chaque groupe de features principale (uniquement sur le train set)\n",
    "- Comme il y a beaucoup d'observation, on pourra faire un KMeans pour afficher les indivius moyen plutot que d'afficher toutes les observations\n",
    "\n",
    "### 8. Preprocessing & Enregistrement des données \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf675b7c",
   "metadata": {},
   "source": [
    "# 1. Decouverte du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\Etude_Data_science\\Kaggle_competition\\04_Predict CO2 Emissions in Rwanda\\dataset\\train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6896e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path)\n",
    "df = data.copy()\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344c77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477b46d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c20e0b",
   "metadata": {},
   "source": [
    "- Avec le head et tail, on peut noter la présence de données manquantes, que le jeu de données continent des informations temporelles (on démarre de l'année 2019 au premier weekend jusqu'en 2021 au weekend 52, soit 53 semaines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116dbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46567da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f086b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Aperçu rapide de quelques mesures statistiques\n",
    "df.describe()\n",
    "#on note la présence de données manquantes (count < nombre de ligne total du jeu de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f65caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Résumer du type de variable présent : \n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb60597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération de la target : \n",
    "target = df.emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupérons l'id du jeu de données \n",
    "identifiant = df.ID_LAT_LON_YEAR_WEEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifions si l'id est unique pour chaque ligne :\n",
    "identifiant.nunique() == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd4b5f",
   "metadata": {},
   "source": [
    "## Premier bilan des observations : \n",
    "- La target est **emission** qui est une variable quantitative continues : **Regression**\n",
    "- Il n'y a que des **variables numériques** : \n",
    "    * 2 discrètes (année : de 2019 a 2021 et le numéro de la semaine (allant de 0 pour la 1ere semaine à 52)\n",
    "    * 73 continues : correspondent aux **features principales et leurs déclinaisons**\n",
    "    * 1 ordinale : correspond à l'id de chaque lignes \n",
    "- Présence de données manquantes\n",
    "- La nomenclature utilisé ici pour les noms de colonnes est **NomFeaturePrincipale_NomFeatureSecondaire** ce qui va nous permettre de créer une fonction qui facilitera la fraction du jeu de données pour pouvoir analyser plus simplement les 75 colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une fonction permettant de lister toutes les colonnes selon un préfix définit : \n",
    "def find_columns(mask,dataframe):\n",
    "    \"\"\"Fonction qui renvoie une liste contenant toutes les colonnes de df commençant par le mask précisé en paramètre\"\"\"\n",
    "    list_col = [i for i in dataframe if i[:len(mask)] == mask]\n",
    "    return list_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f65595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation : Les colonnes avec carbone : \n",
    "col_carbon = find_columns(\"Carbon\",df)\n",
    "df[col_carbon].head() #Toutes les colonnes démarrant avec le mot clef \"Carbon\" seront récupérées "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481447f",
   "metadata": {},
   "source": [
    "Avec cette fonction, nous pourrons plus facilement accéder aux différentes colonnes du jeu de données, sachant que nous avons 7 nom de features principales, nous pourrons afficher plus facilement les colonnes par groupe de features principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef94a07",
   "metadata": {},
   "source": [
    "## 2. imputation des données :\n",
    "NaN présent : vrai NaN ou signifie une absence d'élément ?\n",
    "\n",
    "Comme nous somme sur des variables numériques, 0 représente une quantitée nulle.\n",
    "\n",
    "Pour vérifier cela, regardons simplement si nous avons la présence de la valeur 0 dans le jeu de données. \n",
    "\n",
    "- **Cas 1 : Il n'y a pas de 0 présent dans les colonnes contenant des NaN ET dans les colonnes sans NaN** \n",
    "    - Il y a de forte chance que le NaN soit réellement un NaN et qu'il n'y a tout simplement pas de 0\n",
    "- **Cas 2 : Il y a  des 0 présent dans les colonnes contenant des NaN ET dans les colonnes sans NaN** \n",
    "    - Forte chance que le NaN soit un NaN, la présence de 0 confirme une quantitée nulle, et le NaN lorsque l'info est manquante\n",
    "    \n",
    "- **Cas 3 : on comptabilise des 0 dans les colonnes contenant des NaN**\n",
    "    - S'il y a des 0 en plus des NaN, alors le NaN est une donnée manquante\n",
    "- **Cas 4 : Il n'y a pas de 0 uniquement dans les colonnes contenant des NaN**\n",
    "    - S'il n'y a aucun 0 dans les colonnes contenant des NaN, alors qu'il y a des 0 dans les colonnes ou il n'y a pas de NaN, alors on est pratiquement sûr que les NaN correspondent a 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récuperons les colonnes contenant des NaN\n",
    "col_with_nan = [i for i in df if df[i].isnull().sum()>0]\n",
    "#Colonne sans NaN :\n",
    "col_no_nan = [i for i in df if i not in col_with_nan]\n",
    "\n",
    "print(\"Nombre de colonnes avec des NaN :\", len(col_with_nan))\n",
    "print(\"Nombre de colonnes sans NaN :\", len(col_no_nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa5512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Regardons s'il y a des 0 pour les colonnes contenant au moins 1 NaN :\n",
    "for i in df[col_with_nan]:\n",
    "    compteur_0 = 0\n",
    "    col = df[i]\n",
    "    #somme des 0 de la colonne \n",
    "    cumsum = (col == 0).sum()\n",
    "    if cumsum>0:#si on a au moins un 0 dans la colonne on incrémente le compteur\n",
    "        compteur_0 +=1\n",
    "    if compteur_0>0: #si le compteur a été incrémenter (donc si ya eu des 0), on l'affiche\n",
    "        print(f\" {i:-<35} : {compteur_0}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18536ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Regardons le nombre de 0 pour les colonnes sans NaN : \n",
    "\n",
    "for i in df[col_no_nan]:\n",
    "    compteur_0 = 0\n",
    "    col = df[i]\n",
    "    #somme des 0 de la colonne \n",
    "    cumsum = (col == 0).sum()\n",
    "    if cumsum>0:#si on a au moins un 0 dans la colonne on incrémente le compteur\n",
    "        compteur_0 +=1\n",
    "    if compteur_0>0: #si le compteur a été incrémenter (donc si ya eu des 0), on l'affiche\n",
    "        print(f\" {i:-<35} : {compteur_0}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad21a2",
   "metadata": {},
   "source": [
    "### Bilan : \n",
    "* Pour les colonnes qui ont des NaN, certaines colonnes ont des 0\n",
    "* Pour les colonnes qui n'ont pas de NaN, même observation\n",
    "\n",
    "- Nous somme dans le cas 2, donc ici les NaN sont de vraies données manquantes que nous allons imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e1492",
   "metadata": {},
   "source": [
    "### Visualisation des NaN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(df[col_with_nan])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbfac3",
   "metadata": {},
   "source": [
    "- Nous voyons la proximité des données manquantes, les colonnes partageant le même préfix sont les plus proches entre elles. Par exemple toutes les colonnes commençant par \"NitrogenDioxide\" partagent beaucoup plus de lignes communes avec des NaN que les colonnes commençant par \"Uv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df[col_with_nan], color=\"slateblue\",sort='descending', )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21533c",
   "metadata": {},
   "source": [
    "Nous voyons ici qu'il y a quelques de colonnes contenant beaucoup de données manquantes, nous allons, en fonction de la quantité de données manquantes appliquer différentes stratégies : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94655e39",
   "metadata": {},
   "source": [
    "- **1 :** données avec moins de 40%, on impute\n",
    "- **2 :** données avec plus de 40% de NaN on supprime colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b224f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().mean()[round(df.isnull().mean()*100,2)>40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col_with_nan].isnull().mean()[round(df[col_with_nan].isnull().mean()*100,2)<=40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93db44",
   "metadata": {},
   "source": [
    "a faire :\n",
    "- supprr colonne >40 en rcupérant le nom des colonnes\n",
    "- recup colonne <40 faire limputation par moyenne ou mediane en faisant visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dff51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_inf_40 = list(df[col_with_nan].isnull().mean()[round(df[col_with_nan].isnull().mean()*100,2)<=40].keys())\n",
    "col_sup_40 = list(df[col_with_nan].isnull().mean()[round(df[col_with_nan].isnull().mean()*100,2)>40].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc55721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes contenant trop de données manquantes : \n",
    "df.drop(col_sup_40, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39695992",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_inf_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bddbf",
   "metadata": {},
   "source": [
    "#### Imputation des données \n",
    "- Visualisation des outliers par boxplot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une fonction pour visualiser plusieurs boxplot simultanément \n",
    "def multi_boxplot(columns, nrow, ncol):\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(16, 12))\n",
    "    for i, k in enumerate(columns):\n",
    "        row = i // ncol\n",
    "        col = i % ncol  # Correction ici\n",
    "        ax = sns.boxplot(data=df, y=k, ax=axes[row, col], color =\"skyblue\")\n",
    "        ax.set_ylabel('')# Suppression du ylabel\n",
    "        ax.set_xlabel(k, rotation = 0,fontsize=6, fontweight='bold') #on le met plutot sur l'axe x\n",
    "    plt.show()  # plt.show() en dehors de la boucle for pour afficher tous les boxplots en une seule fois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a05a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boxplot(col_inf_40[:20],4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de800aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boxplot(col_inf_40[20:40],4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boxplot(col_inf_40[40:],5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa50cc",
   "metadata": {},
   "source": [
    "#### Détection d'outliers  par méthode de calcul :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detecter_outliers(column):\n",
    "    \"\"\"Fonction qui calcul la quantité d'outlier d'une colonne donnée\"\"\"\n",
    "    Q1 = column.quantile(0.25) #récupération des quantiles Q1 Q3\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1  #Calcul interquantile\n",
    "    limite_inferieure = Q1 - 1.5 * IQR #on définit les bornes inférieur et supérieur\n",
    "    limite_superieure = Q3 + 1.5 * IQR\n",
    "    \n",
    "    #Somme des outliers\n",
    "    nombre_outliers = ((column < limite_inferieure) | (column > limite_superieure)).sum()\n",
    "    \n",
    "    #Calcul du % d'outliers sur la colonne\n",
    "    pourcentage_outliers = (nombre_outliers / len(column)) * 100\n",
    "    \n",
    "\n",
    "    #renvoi du % arrondie au 10e\n",
    "    return round(pourcentage_outliers,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in col_inf_40:\n",
    "    # Utilisation de la fonction pour détecter le pourcentage d'outliers dans une colonne\n",
    "    pourcentage_outliers = detecter_outliers(df[i])\n",
    "    if pourcentage_outliers>5:\n",
    "        print(f\"% Outlier {i} : {pourcentage_outliers}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122c25f",
   "metadata": {},
   "source": [
    "- hormis pour 1 features, il n'y a pas de features présentant un % outliers > 5\n",
    "- On va remplacer par la **moyenne** pour chaque variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation des données par Imputer :\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "imputer.fit(df[col_inf_40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation des données : \n",
    "imputed_data = imputer.transform(df[col_inf_40])\n",
    "#remplacement des données imputées dans le jeu de données  :\n",
    "df[col_inf_40] = pd.DataFrame(imputed_data, columns=col_inf_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74110e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vérifions s'il n'y a plus de données manquantes dans le jeu de données :\n",
    "df.isnull().sum()[df.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74664d59",
   "metadata": {},
   "source": [
    "## 3. Analyse univariée :\n",
    "\n",
    "- Nous avons en quelque sorte démarré cette étape lors de l'imputation en affichant des boxplots.\n",
    "- Nous pouvons aussi regarder la distribution des données  \n",
    "    - Affichons la distribution\n",
    "    - A l'aide d'un pairplot nous regarderons la distribution des données \n",
    "    - Nous regarderons la distribution de la target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_distrib(columns, nrow, ncol):\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(20, 16))\n",
    "    for i, k in enumerate(columns):\n",
    "        row = i // ncol\n",
    "        col = i % ncol  # Correction ici\n",
    "        ax = sns.distplot(a=df[k], ax=axes[row, col], color =\"pink\", kde_kws={\"color\":\"red\"})\n",
    "        ax.axvline(df[k].mean(), label = \"mean\", ls=\"--\",color = \"orange\")\n",
    "        ax.axvline(df[k].median(),ls= \":\", label = \"median\", color = \"blue\")\n",
    "        ax.legend()\n",
    "#         ax.set_xlabel('')# Suppression du xlabel\n",
    "#         ax.set_ylabel(k, rotation = 0,fontsize=6, fontweight='bold') #on le met plutot sur l'axe x\n",
    "    plt.show()  # Déplacez ce plt.show() en dehors de la boucle for pour afficher tous les boxplots en une seule fois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b99304",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = df.columns[1:20]  #démarre par 1 pour exclure la premiere colonne qui est l'ID\n",
    "col2 = df.columns[20:40]\n",
    "col3 = df.columns[40:60]\n",
    "col4 = df.columns[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_distrib(col1,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_distrib(col2,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ad73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_distrib(col3,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ced6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_distrib(col4,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c5485",
   "metadata": {},
   "source": [
    "Au vu de la quantité d'observation, nous allons réduire notre jeu de données pour faciliter sa visualisation\n",
    "- prenons 1000observations de celui ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frac = df.sample(n=1000,random_state=42)\n",
    "df_frac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb259cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_frac[col1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf52fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_frac[col2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d0b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_frac[col3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d95c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_frac[col4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9463e7",
   "metadata": {},
   "source": [
    "- La distribution des données entre la target et les quelques features sur le 4è pairplot est **non linéaire**, donc lors du heatmap il y a de forte chance qu'on observe de faible corrélation entre la target et les différentes features\n",
    "\n",
    "- Ensuite, on peut voir qu'il y a quelque features corrélées entre elles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e38da",
   "metadata": {},
   "source": [
    "#### variables discrètes :\n",
    "- weekno & year sont les deux variables discrètes et n'ont pas besoin de visualisation, il n'y a qu'un weekend par ligne et que 3 années différentes équitablement réparties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12286c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_x = df.year.value_counts().keys()\n",
    "year_y = df.year.value_counts().values\n",
    "sns.barplot(data = df, x = year_x , y=year_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018632a",
   "metadata": {},
   "source": [
    "#### Analyse target :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(target, color='skyblue', edgecolor='black', bins = 20)\n",
    "plt.xlabel('Emission :')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution de la variable cible : Emission')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c2d24",
   "metadata": {},
   "source": [
    "On peut observer une distribution multimodal avec un premier pic majoritaire (frequence >60.000) suivi d'un pic  ~ 10.000 et enfin un dernier pic notable  ~ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'histogramme de la target\n",
    "occurence, bin_edges = np.histogram(target, bins=20)  #mettons 20 bins comme sur la figure\n",
    "\n",
    "#Affichons les 3 premier intervalle pour nos 3 modes principaux : \n",
    "print(\"Limites des 3 premiers bins : \")\n",
    "print(bin_edges[:3])\n",
    "print()\n",
    "print(\"Nombre d'occurence pour les 3 premiers bins :\")\n",
    "print(occurence[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461dcb2",
   "metadata": {},
   "source": [
    "### Analyse bivariée : \n",
    "\n",
    "-  Nous avons vu qu'il y avait peu de corrélation linéaire lorsque la target était impliquée.\n",
    "- Nous allons faire une heatmap et regarder la corrélation linéaire de plus près entre la target et les différentes features\n",
    "- Si, effectivement nous apercevons très peu de corrélation linéaire, nous devrons utiliser non pas le test de pearson mais celui de **Spearman** (préférable au test de Kendall au vu de la quantitée de données importantes ici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = list(df.columns[1:20])  #démarre par 1 pour exclure la premiere colonne qui est l'ID\n",
    "col2 = list(df.columns[20:40])\n",
    "col3 = list(df.columns[40:60])\n",
    "col4 = list(df.columns[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(identifiant.name, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47a8ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sélection des variables continues de votre jeu de données\n",
    "var_cont = list(df.drop([\"week_no\",\"year\",\"emission\"], axis = 1).columns)\n",
    "# Calcule de la corrélation entre la target et chaque variable continue\n",
    "correlation = df[var_cont ].corrwith(df['emission']) #corrwith() permet de calculer la corrélation entre\n",
    "# Trier les valeurs de corrélation par ordre décroissant\n",
    "sorted_correlation = correlation.sort_values(ascending=False)\n",
    "#la target et chaque variable continue\n",
    "plt.figure(figsize=(10, 20))\n",
    "heatmap = sns.heatmap(pd.DataFrame(sorted_correlation), annot=True, cmap='RdYlGn',fmt=\".2%\", cbar=True)\n",
    "heatmap.set_title('Corrélation entre la target et les variables continues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11156a",
   "metadata": {},
   "source": [
    "Nous confirmons notre hypothèse précédente, à savoir **qu'il y a une faible corrélation linéaire entre notre target et nos features**\n",
    "\n",
    "- Nous allons faire un test de shapiro wilk et en fonction du résultat nous allons utiliser le test statistique adapté : \n",
    "    - Les features suivant une loi normal : on fera le test de **pearson**\n",
    "    - Les features ne suivant pas une loi normal : on fera le test de **Kendall** (et non spearman car on a dit ici qu'il y avait beaucoup de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96731d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fonction pour vérifier si une variable suit une loi normale : \n",
    "def shapiro_test(list_col, dataframe):\n",
    "    \"\"\"\n",
    "    Fonction shapiro test qui renvoie une liste \n",
    "    \n",
    "    \"\"\"\n",
    "    if type(list_col) == str:\n",
    "        list_col = [list_col]\n",
    "    \n",
    "    var_normal = []\n",
    "    var_non_normal = []\n",
    "    \n",
    "    # Test de normalité de Shapiro-Wilk\n",
    "    for i in list_col:\n",
    "        statistic, p_value = shapiro(dataframe[i])\n",
    "#         print(\"feature :\", i)\n",
    "    \n",
    "        # Afficher les résultats\n",
    "#         print(f\"Statistique de test W : {round(statistic,2)}\")\n",
    "#         print(f\"P-value : {p_value}\")\n",
    "    \n",
    "    # Interpréter les résultats\n",
    "        alpha = 0.05  # Niveau de signification\n",
    "    \n",
    "        if p_value < alpha:\n",
    "#             print(\"Les données ne suivent probablement pas une distribution normale.\")\n",
    "            var_non_normal.append(i)\n",
    "        else:\n",
    "#             print(\"Les données semblent suivre une distribution normale.\")\n",
    "            var_normal.append(i)\n",
    "#         print()    \n",
    "    \n",
    "#     print(\"Features suivant une loi normale :\", var_normal)\n",
    "#     print(\"Features ne suivant pas une loi normale :\", var_non_normal)\n",
    "    return var_normal, var_non_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On va créer une liste contenant l'ensemble des features a l'exception de la target\n",
    "all_features = [i for i in  list(df.columns) if i !=  target.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e831e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_norm, var_non_norm = shapiro_test(all_features, dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b150801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features suivant une loi normale : \\n\", var_norm)\n",
    "print()\n",
    "print(\"Features ne suivant pas une loi normale :\\n\", var_non_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa7394",
   "metadata": {},
   "source": [
    "Aucune variable ne suit une loi normale donc nous allons utiliser uniquement le test de kendall :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd442b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_test(feature, target, dataframe):\n",
    "    \"\"\"\n",
    "    Fonction pour effectuer le test de corrélation de Kendall entre une feature et une target.\n",
    "    \n",
    "    Paramètres :\n",
    "    - feature : Liste de features ou une feature unique\n",
    "    - target : Nom de la target\n",
    "    - dataframe : DataFrame contenant les données\n",
    "    \n",
    "    Retourne :\n",
    "    - var_a_conserver : Liste des features ayant une corrélation significative avec la target\n",
    "    - var_a_suppr : Liste des features n'ayant pas une corrélation significative avec la target\n",
    "    \"\"\"\n",
    "    var_a_conserver = []\n",
    "    var_a_suppr = []\n",
    "\n",
    "    # Test de corrélation de Kendall\n",
    "    for feat in feature:\n",
    "        corr, p_value = kendalltau(dataframe[feat], dataframe[target])\n",
    "\n",
    "        # Afficher les résultats\n",
    "#         print(f\"Feature : {feat}\")\n",
    "#         print(f\"Corrélation avec {target} : {corr}\")\n",
    "#         print(f\"P-value : {p_value}\")\n",
    "\n",
    "        # Interpréter les résultats\n",
    "        alpha = 0.05  # Niveau de signification\n",
    "        if p_value < alpha:\n",
    "            var_a_conserver.append(feat)\n",
    "        else:\n",
    "            var_a_suppr.append(feat)\n",
    "#         print()\n",
    "\n",
    "#     print(\"Features à conserver :\", var_a_conserver)\n",
    "#     print(\"Features à supprimer :\", var_a_suppr)\n",
    "    return var_a_conserver, var_a_suppr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40925730",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_a_conserver, var_a_supprimer  =kendall_test(all_features, \"emission\", dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e467e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Liste des features à conserver : \\n\",  var_a_conserver)\n",
    "print()\n",
    "print(\"Liste des features à supprimer : \\n\",  var_a_supprimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(var_a_supprimer, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb362a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des variables discrètes\n",
    "var_dis = [i for i in df if (i not in var_cont) & (i != target.name)]\n",
    "var_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c611688",
   "metadata": {},
   "source": [
    "# PCA & tSNE : Sur le train set\n",
    "- Au vu des données, nous avons remarqué qu'il y avait peu de linéarité lors des pairplot, ainsi que lors de la heatmap , nous allons donc en plus de faire une PCA, une tSNE afin d'avoir une meilleur appréciation des données.\n",
    "\n",
    "- Pour cela, nous allons demarrer avec une PCA, nous ferons le cercle des corrélation et nous utiliserons aussi ce cercle pour nous aider à interpréter les données que nous obtiendrons après une réduction par tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e69bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df.drop([\"emission\"], axis = 1), df[\"emission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions des données :\\n\")\n",
    "print('X_train :', X_train.shape)\n",
    "print('y_train :', y_train.shape)\n",
    "print()\n",
    "print('X_test :', X_test.shape)\n",
    "print('y_test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67cc88",
   "metadata": {},
   "source": [
    "### Standardisation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977610f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[var_cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7968f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réduction des données :\n",
    "X_train_reduced = pd.DataFrame(scaler.transform(X_train[var_cont]), columns=var_cont, index = X_train.index)\n",
    "X_test_reduced = pd.DataFrame(scaler.transform(X_test[var_cont]), columns=var_cont, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concaténation de l'ensemble des features réduites : \n",
    "X_reduced = pd.concat([X_train_reduced, X_test_reduced])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5f535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = pd.concat([X_reduced,df[var_dis],y], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c50e5",
   "metadata": {},
   "source": [
    "### PCA : Sur l'ensemble du jeu de données (cad sur toutes les colonnes, mais sur le train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10, random_state=42) #gardons les 10 premieres composantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.fit_transform(X_train_reduced[var_cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformons les données pca directement en un dataframe\n",
    "#ou les colonnes sont les components principales \n",
    "#et les lignes les individus \n",
    "df_x_pca = pd.DataFrame(x_pca, columns=[f\"F{i+1}\" for i in range(pca.n_components_)],index=X_train_reduced.index)\n",
    "df_x_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374077c8",
   "metadata": {},
   "source": [
    "* Pour rappel les components principales sont le résultat de l'expression de chacune des features de bases\n",
    "* F1 provient de l'ensemble de la somme des produits des features initiales. \n",
    "#### Nous y reviendrons ultérieurement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recuperons les indices des principals components\n",
    "n_components = [i for i in range(pca.n_components_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5353ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Récupération de la variance (autrement dit l'information conservée au travers chaque components principales)\n",
    "\n",
    "#Calcul de la variance cumulée en %\n",
    "sse = pca.explained_variance_ratio_*100\n",
    "#Calcul de la somme cumulée de chaque variance :\n",
    "sse_cum = [round(i,2) for i in np.cumsum(sse)]\n",
    "sse_cum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a95da",
   "metadata": {},
   "source": [
    "### Regardons graphiquement ce que chaque composantes principale apporte comme information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x = n_components, y=sse_cum, markers=\"d\", color = \"red\",style=True)\n",
    "ax = sns.barplot(x = n_components, y=sse,)\n",
    "ax.get_legend().remove()#suppression de la legende pour le markers du lineplot\n",
    "ax.set_xticklabels([i+1 for i in n_components])#pour commencer a compter à partir de 1 sur le graph\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"On passe de {pca.n_features_} à {pca.n_components_} features\")\n",
    "print(f\"soit une réduction de {pca.n_features_-pca.n_components_} variables \")\n",
    "print(f\"Nous perdons {round((100-sse_cum[-1]))} % d'informations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932ebb6",
   "metadata": {},
   "source": [
    "#### Regardons la relation entre les principal components et les features de bases :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db70df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcs = pd.DataFrame(data = pca.components_, \n",
    "                   columns=X_reduced.columns,\n",
    "                  index = [f\"F{i+1}\" for i in range(pca.n_components_)]\n",
    "                  )\n",
    "                  \n",
    "df_pcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inversons le dataframe pour faciliter la lisibilité: \n",
    "df_pcs = df_pcs.T\n",
    "df_pcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichons une heatmap pour améliorer le confort de lecture : \n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.title(\"Relation entre les variables synthéthique et les variables initiales\",fontdict={\"color\":\"red\"})\n",
    "sns.heatmap(pcs.T.sort_values(by=\"F1\", ascending=False), annot = True, linecolor=\"black\", linewidths=0.5, cmap=\"RdYlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a976a",
   "metadata": {},
   "source": [
    "- La heatmap nous permet de voir les relations entre les variables initiales et les variables synthéthiques. \n",
    "\n",
    "* Comme dit plus haut  avec le dataframe **df_x_pca** ou nous avons les individus en lignes et en colonnes les nouvelles composantes.\n",
    "- Pour mieux visualiser la relation entre les composantes principales et les variables initiales, nous pouvons tracer le cercle des corrélation : \n",
    "\n",
    "- Pour mieux visualiser la relation entre les composantes principales et les variables initiales, nous pouvons tracer le cercle des corrélation : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852631c",
   "metadata": {},
   "source": [
    "### Cercle des corrélation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_graph(pca, \n",
    "                      x_y, \n",
    "                      features) : \n",
    "    \"\"\"Affiche le graphe des correlations\n",
    "    Positional arguments : \n",
    "    -----------------------------------\n",
    "    pca : sklearn.decomposition.PCA : notre objet PCA qui a été fit\n",
    "    x_y : list ou tuple : le couple x,y des plans à afficher, exemple [0,1] pour F1, F2\n",
    "    features : list ou tuple : la liste des features (ie des dimensions) à représenter\n",
    "    \"\"\"\n",
    "    # Extrait x et y \n",
    "    x,y=x_y\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "    # Pour chaque composante : \n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "        # Les flèches\n",
    "        ax.arrow(0,0, \n",
    "                pca.components_[x, i],  \n",
    "                pca.components_[y, i],  \n",
    "                head_width=0.07,\n",
    "                head_length=0.07, \n",
    "                width=0.02, )\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                pca.components_[y, i] + 0.05,\n",
    "                features[i])      \n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x],1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y],1)))\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1))\n",
    "   \n",
    "    # Le cercle \n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0597687",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_graph(pca, (1,2), var_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f698d02",
   "metadata": {},
   "source": [
    "- On regarde le cercle des corrélation en parallèle avec la heatmap ci dessus pour mieux comprendre\n",
    "- certaines variables partageant des caractéristique communes se trouvent dans les même direction (colorisée en  orange sur F1 de la heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35f926",
   "metadata": {},
   "source": [
    "### utilisation algorithme de clustering\n",
    "- Nous allons utiliser un Kmeans afin de mieux voir les clusters avec de nouveaux label pour mieux apprecier les nouveaux clusters qui se dessinent (s'il y en a). Le but de faire ce clustering c'est de voir si on ne peut pas attribuer de nouveaux labels a ces individus.\n",
    "- De plus, nous afficherons aussi le graphique des individus afin de voir si on peut interpréter les clusters d'individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_elbow(data, nclust):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    visualizer = KElbowVisualizer(KMeans(), k=(1, nclust))  # Spécifiez la plage de k que vous souhaitez explorer\n",
    "    visualizer.fit(data)\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7751c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_elbow(df_x_pca, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1852d1",
   "metadata": {},
   "source": [
    "On voit 3 clusters sur les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des clusters avec colorisation en fonction de la target\n",
    "sns.scatterplot(x=df_x_pca.iloc[:, 0], y=df_x_pca.iloc[:, 1], hue=y_train, palette=\"viridis\")\n",
    "plt.title('Clusters colorisés en fonction de la target (sans kmeans)')\n",
    "plt.xlabel('F1')\n",
    "plt.ylabel('F2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c85d81",
   "metadata": {},
   "source": [
    "- On voit ici que les 2 premiers plans sont mal capturé, en effet on se retrouve avec un nuage de point très dense ne laissant apparaitre aucun cluster.\n",
    "- La qualité d'information, la quantité de features ainsi que le nombre d'observation qui rend toutes ces relations complexes, non linéaire sont a l'origine de cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_kmeans(data, cluster,x,y):\n",
    "    kmeans = KMeans(n_clusters=cluster, random_state=42)  \n",
    "    kmeans.fit(data)  \n",
    "\n",
    "# labels de cluster attribués à chaque point de données\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "#attribuons un nom pour les labels de la sur graphique : \n",
    "    lab_graph = [F\"Cluster {i}\" for i in labels]\n",
    "    plt.title(\"Données colorisées après un kmeans\")\n",
    "    sns.scatterplot(x=data.iloc[:, x], y=data.iloc[:, y],\n",
    "                hue=lab_graph, #hue pour donner le nom des clusters\n",
    "                hue_order=np.unique(lab_graph), #l'ordre dans lequel on veut afficher les noms grâce à la légende\n",
    "                palette=\"bright\")\n",
    "    plt.legend(bbox_to_anchor=(1,1))\n",
    "    plt.xlabel(f\"F{x}\")\n",
    "    plt.ylabel(f\"F{y}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7477fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_kmeans(data=df_x_pca, cluster=3,x=0,y=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23054f54",
   "metadata": {},
   "source": [
    "- On peut voir ici une grosse difficulté a séparer les clusters, toutes les données sont mélangées\n",
    "- C'était attendu, en effet, nous savons que nos données présentent des relations complexes entre elles avec peut de linéarité, nous avons des données complexes, ainsi, il serait peut être plus pertinent d'utiliser une réduction dimensionnalité avec un tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c16c8c",
   "metadata": {},
   "source": [
    "### tSNE : Sur l'ensemble des features du jeu de données (mais train set)\n",
    "- Attention cet algorithme est gourmand et nécessite demande beaucoup plus de temps s'il y a un nombre important de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = tsne.fit_transform(X_train_reduced[var_cont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_elbow(X_tsne, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddf1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des clusters avec colorisation en fonction de la target\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y_train, palette=\"viridis\")\n",
    "plt.title('Clusters colorisés en fonction de la target (sans kmeans)')\n",
    "plt.xlabel('F1')\n",
    "plt.ylabel('F2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_kmeans(data=X_tsne, cluster=3,x=0,y=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35214c20",
   "metadata": {},
   "source": [
    "- La séparation des clustersr est légèrement améliorée, surtout entre le cluster 0 et 1 mais ca reste encore complexe\n",
    "- la PCA et le tSNE ne sont pas convaincant, tentons une autre approche "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b006240",
   "metadata": {},
   "source": [
    "### 8. PCA & tSNE : En fonction de chaque features principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd52431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Nous avons en tout\", df_reduced.shape[1] ,\"variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0b6b2",
   "metadata": {},
   "source": [
    "Ces variables sont catégorisée selon la hiérarchie suivante suivante : Une feature principale est accompagnée de X sous features.\n",
    "- Nous avons 7 features principales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476381c6",
   "metadata": {},
   "source": [
    "\n",
    "- **SulphurDioxide** : pénètre dans l'atmosphère terrestre par des processus naturels et anthropiques.\n",
    "- **CarbonMonoxide** : Dans certaines zones urbaines, il constitue un polluant atmosphérique majeur\n",
    "- **NitrogenDioxide** : NO2 est utilisé pour représenter les concentrations d'oxydes d'azote collectifs car pendant la journée, c'est-à-dire en présence de lumière solaire, un cycle photochimique impliquant l'ozone (O3) convertit NO en NO2 et vice-versa sur une échelle de temps de quelques minutes.\n",
    "- **Formaldehyde** : Le formaldéhyde est un gaz intermédiaire dans presque toutes les chaînes d'oxydation des composés organiques volatils non méthaniques (COVNM), qui aboutissent finalement au CO2.\n",
    "- **UvAerosolIndex** :l'indice d'aérosol UV (UVAI), également appelé indice d'aérosol absorbant (AAI).Lorsque l'IAA est positif, il indique la présence d'aérosols absorbant les UV, tels que la poussière et la fumée.\n",
    "- **Ozone** : Dans la stratosphère, la couche d'ozone protège la biosphère des dangereux rayonnements solaires ultraviolets. Dans la troposphère, elle agit comme un agent nettoyant efficace, mais à forte concentration, elle devient également nocive pour la santé de l'homme, des animaux et de la végétation. C'est un important gaz à effet de serre qui contribue au changement climatique\n",
    "- **Cloud** : Ce jeu de données récupères les propriétés des nuages et fournit une imagerie hors ligne à haute résolution de ses paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd09f5",
   "metadata": {},
   "source": [
    "récupérons ces différentes features principales et leurs sous features dans différentes variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dioxyde_soufre = find_columns(\"Sulphu\",df)\n",
    "monoxyde = find_columns(\"Carbo\",df)\n",
    "nitrogen = find_columns(\"Nitrogen\",df)\n",
    "formaldehyde = find_columns(\"Formaldehyde\",df)\n",
    "uvaerosol = find_columns(\"UvAerosolIndex\",df)\n",
    "ozone = find_columns(\"Ozone\",df)\n",
    "cloud = find_columns(\"Cloud\",df)\n",
    "\n",
    "\n",
    "all_type_col = [dioxyde_soufre, monoxyde, nitrogen, formaldehyde, uvaerosol, ozone,cloud]\n",
    "dict_main_col = {\"dioxyde_soufre\":dioxyde_soufre,\n",
    "               \"monoxyde\":monoxyde,\n",
    "               \"nitrogen\":nitrogen,\n",
    "               \"formaldehyde\":formaldehyde,\n",
    "               \"uvaerosol\":uvaerosol,\n",
    "               \"ozone\":ozone,\n",
    "               \"cloud\":cloud}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_type_col[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_col = [len(i) for i in all_type_col]\n",
    "sum(len_col) #il manque 5 colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f701e0a",
   "metadata": {},
   "source": [
    "#### Repérons les colonnes manquantes : \n",
    "- Pour cela, nous allons deja faire une seule liste, car le probleme avec notre variable all_type_col c'est que c'est une liste qui contient des sous liste de chaque features qui démarre par le préfix donné par notre fonction find_columns\n",
    "- Or nous voulons faire une seule liste qui contiendra toutes les features, pour cela nous allons l'\"aplatir\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46916ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_aplatie = []\n",
    "# Aplatissons notre liste en ajoutant chaque élément des sous-listes à la liste aplatie\n",
    "for sous_liste in all_type_col:\n",
    "    liste_aplatie.extend(sous_liste)\n",
    "\n",
    "liste_aplatie[:5] #nous voyons que nous n'avons plus de sous liste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9a5c8",
   "metadata": {},
   "source": [
    "Regardons quelles features est passé entre les mailles du filet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in df if i not in liste_aplatie] #on regarde les colonnes qui sont sur df mais qu'on a pas su récupérer \n",
    "#avec l'utilisation de notre fonction find_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd3ced",
   "metadata": {},
   "source": [
    "- heuresement ! Ce sont les features qui ne font pas parti des 7 features principales, nous avons la target, les coordonnées GPS, l'année et le week-end\n",
    "\n",
    "* Pour nos 7 ACP, nous allons pouvoir utiliser chacune des variables contenant l'ensemble des sous features.\n",
    "- Exemple avec dioxyde_soufre = find_columns(\"Sulphu\",df), nous allons faire une ACP contenant toutes les features avec le soufre\n",
    "\n",
    "### L'objectif est de faire autant d'ACP pour chaque feature principale afin de réduire le nombre de dimension (de feature) puis ensuite de concaténer toutes ces features ensemble afin d'avoir un jeu de donnée réduit.\n",
    "### Cela rendra la démarche plus lisible que notre premiere tentative ou nous avons fait une ACP directement sur tous le jeu de données !\n",
    "\n",
    "Nous pourrons visualiser le cercle des corrélation ainsi que le graphique des individus pour chaque ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdeaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des noms que nous avons donné pour chaque feature principale\n",
    "name_main_feat = [i for i in dict_main_col.keys()]\n",
    "name_main_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbaab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_main_col[name_main_feat[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110759f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instant_columns(indice, voir_liste=False):\n",
    "    \"\"\"\n",
    "    fonction qui permet d'acceder directement a l'ensemble des sous\n",
    "    features d'une features principal en indexant une valeur contenue dans \n",
    "    notre liste des noms de features principales\n",
    "    \"\"\"\n",
    "    if voir_liste:\n",
    "        print(name_main_feat)\n",
    "    \n",
    "    taille_ind = len(name_main_feat)\n",
    "    if indice not in range(0,len(name_main_feat)):\n",
    "        print(\"l'indice doit se situer entre \",min(range(taille_ind)), \"et\", max(range(taille_ind)))\n",
    "    return dict_main_col[name_main_feat[indice]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f177fcd",
   "metadata": {},
   "source": [
    "Exemple d'utilisation :\n",
    "- on affiche la liste contenant le nom des features principal\n",
    "- Le 1er element de la liste est dioxyde_soufre, donc si on met l'indice 0 on affichera tous les noms de sous features contenant \"SulphurDioxide\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "instant_columns(0,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8fc88",
   "metadata": {},
   "source": [
    "## 8. PCA \n",
    "- Faisons une PCA pour les 7 features principales\n",
    "\n",
    "#### 1er groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_pca(indice, n_components=0.8):\n",
    "    \"\"\"\n",
    "    indice : int ; \n",
    "    fonction  qui renvoi un tuple de deux dataframe et de la pca :\n",
    "    1er dataframe contient :\n",
    "    les individus en ligne et les principal components en colonne\n",
    "    2e dataframe contient :\n",
    "    Les principal components en ligne et les features initiale\n",
    "    \"\"\"\n",
    "    #verification de la condition de l'indice\n",
    "    taille_ind = len(name_main_feat)\n",
    "    if indice not in range(0,len(name_main_feat)):\n",
    "        print(\"l'indice doit se situer entre \",min(range(taille_ind)), \"et\", max(range(taille_ind)))\n",
    "    \n",
    "    #Mise ne place de la PCA : \n",
    "    pca = PCA(n_components=n_components, random_state = 42)\n",
    "    x_pca = pca.fit_transform(X_train_reduced[instant_columns(indice)])\n",
    "    #Récupération des indices de chaque composants principales Fn..\n",
    "    n_components = [i for i in range(pca.n_components_)]\n",
    "\n",
    "    #Création du dataframe individus/principale composantes\n",
    "        \n",
    "    df_x_pca = pd.DataFrame(x_pca, \n",
    "    columns=[f\"F{i+1}_{z.split('_')[0]}\"  #création d'un index unique sous la forme Fn_nom_feature_principal\n",
    "                           for (i,z) in zip(range(pca.n_components_), instant_columns(indice)) ],\n",
    "                            \n",
    "    index=X_train_reduced.index\n",
    "    )\n",
    "    \n",
    "    #Création du dataframe variables initiales/principale composantes\n",
    "    \n",
    "    df_pcs = pd.DataFrame(data = pca.components_, \n",
    "                   columns=instant_columns(indice),\n",
    "                  index = [f\"F{i+1}_{z.split('_')[0]}\"  #création d'un index unique sous la forme Fn_nom_feature_principal\n",
    "                           for (i,z) in zip(range(pca.n_components_), instant_columns(indice)) ]\n",
    "                  )\n",
    "    df_pcs = df_pcs.T\n",
    "    return (df_x_pca, df_pcs, pca)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fe727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm(pcs):\n",
    "    # heatmap pour améliorer le confort de lecture : \n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(\"Relation entre les variables synthéthique et les variables initiales\",fontdict={\"color\":\"red\"})\n",
    "    ax = sns.heatmap(pcs, annot = True, linecolor=\"black\", fmt=\".2\", linewidths=0.5, cmap=\"RdYlGn\")\n",
    "    ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=45, fontsize=6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb383c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screeplot_pca(pca):\n",
    "    \n",
    "   #Récupération de la variance (autrement dit l'information conservée au travers chaque components principales)\n",
    "\n",
    "#Calcul de la variance cumulée en %\n",
    "    sse = pca.explained_variance_ratio_*100\n",
    "    #Calcul de la somme cumulée de chaque variance :\n",
    "    sse_cum = [round(i,2) for i in np.cumsum(sse)]\n",
    "    \n",
    "    #Récupération des indices de chaque composants principales Fn..\n",
    "    n_components = [i for i in range(pca.n_components_)]\n",
    "    \n",
    "    sns.reset_defaults()\n",
    "    #Tracer de figure :\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title(f\"Graphique des éblouies des sommes cumulées de la variance\")\n",
    "    sns.lineplot(x = n_components, y=sse_cum, markers=\"o\", color = \"red\",style=True)\n",
    "        # Ajout des valeurs de sse_cum à chaque point du markers du lineplot\n",
    "    for i, txt in enumerate(sse_cum):\n",
    "        plt.text(n_components[i], sse_cum[i], f'{txt}%')\n",
    "\n",
    "    #Tracer du barplot des sommes cumulées:\n",
    "    ax = sns.barplot(x = n_components, y=sse)\n",
    "    ax.get_legend().remove()#suppression de la legende pour le markers du lineplot\n",
    "    ax.set_xticklabels([i+1 for i in n_components])#pour commencer a compter à partir de 1 sur le graph\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(f\"On passe de {pca.n_features_} à {pca.n_components_} features\")\n",
    "    print(f\"soit une réuction de {pca.n_features_-pca.n_components_} variables \")\n",
    "    print(f\"Nous perdons {round((100-sse_cum[-1]))} % d'informations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80006bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_individus(df_pca):\n",
    "    # Affichage des clusters avec colorisation en fonction de la target\n",
    "    sns.scatterplot(x=df_pca.iloc[:, 0], y=df_pca.iloc[:, 1], hue=y_train, palette=\"viridis\")\n",
    "    plt.title('Clusters colorisés en fonction de la target (sans kmeans)')\n",
    "    plt.xlabel('F1')\n",
    "    plt.ylabel('F2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sulphur_pca, df_sulphur_pcs, pca_sulphur = quick_pca(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c62c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "screeplot_pca(pca_sulphur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm(df_sulphur_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de930a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_graph(pca_sulphur,(0,1), instant_columns(0))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d187b7e",
   "metadata": {},
   "source": [
    "pca_individus(df_sulphur_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_elbow(X_train_reduced[instant_columns(0)], nclust=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_kmeans(df_sulphur_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f35e1c",
   "metadata": {},
   "source": [
    "#### 2e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carbon_pca, df_carbon_pcs, pca_carbon = quick_pca(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d10b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30fb09",
   "metadata": {},
   "source": [
    "#### 3e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nitrogen_pca, df_nitrogen_pcs, pca_nitrogen = quick_pca(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb61d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c77dbd",
   "metadata": {},
   "source": [
    "#### 4e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formaldehyde_pca, df_formaldehyde_pcs, pca_formaldehyde = quick_pca(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8019223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922baa1a",
   "metadata": {},
   "source": [
    "#### 5e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e69ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uv_pca, df_uv_pcs, pca_uv = quick_pca(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae2df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2b7bf1e",
   "metadata": {},
   "source": [
    "#### 6e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ozone_pca, df_ozone_pcs, pca_ozone = quick_pca(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147096c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4781e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d338cc6a",
   "metadata": {},
   "source": [
    "#### 7e groupe de feature principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cloud_pca, df_cloud_pcs, pca_cloud = quick_pca(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c47b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "screeplot_pca(pca_carbon)\n",
    "hm(df_carbon_pcs)\n",
    "correlation_graph(pca_carbon,(0,1), instant_columns(1))\n",
    "Kmeans_elbow(X_train_reduced[instant_columns(1)], nclust=5)\n",
    "graph_kmeans(df_carbon_pca, 3, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafbc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
